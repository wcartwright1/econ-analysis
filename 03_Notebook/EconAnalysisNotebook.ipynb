{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os.path\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as pgo\n",
    "\n",
    "from tkinter import *\n",
    "from tkinter import ttk\n",
    "from tkinter import messagebox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs for use in Jupyter notebook when not using application for testing or review\n",
    "baseLocation = r'C:\\Users\\billy\\Documents\\00_presentationEcon'\n",
    "inputFile1 = r'PNRGINDEXM'\n",
    "inputFile2 = r'PINDUINDEXM'\n",
    "inputFileDesc1 = r'Global price of Energy index'\n",
    "inputFileDesc2 = r'Global price of Industrial Materials index'\n",
    "lagPeriods = 6\n",
    "leadPeriods = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetParameters:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def source_user_input_string(self, user_input_string):\n",
    "        '''\n",
    "        Source the inputs from the user and store in a variable to use when 1) creating file paths 2) Labeling charts or tables.\n",
    "        '''\n",
    "        result_user_input_string = str(user_input_string.get())\n",
    "        \n",
    "        return result_user_input_string\n",
    "    \n",
    "    def source_user_input_integer(self, user_input_string):\n",
    "        '''\n",
    "        Source the inputs from the user and store in a variable to use when 1) creating file paths 2) Labeling charts or tables.\n",
    "        '''\n",
    "        result_user_input_integer = int(user_input_string.get())\n",
    "        \n",
    "        return result_user_input_integer\n",
    "\n",
    "    def define_file_path(self, base_location_value, folder_prefix):\n",
    "        '''\n",
    "        Used to create an initial file path based on the passed user inputs.\n",
    "        '''\n",
    "        result_file_path = f'{base_location_value}\\\\{folder_prefix}'\n",
    "        \n",
    "        return result_file_path\n",
    "      \n",
    "    def define_full_file_path(self, base_location_value, folder_prefix, file_name, file_type):\n",
    "        '''\n",
    "        Used to create a complete file path with file names and file types based on the passed user inputs.\n",
    "        '''\n",
    "        result_full_file_path = f'{base_location_value}\\\\{folder_prefix}\\\\{file_name}.{file_type}'\n",
    "        \n",
    "        return result_full_file_path\n",
    "    \n",
    "    def define_full_output_file_path_detailed(self, base_location_value, folder_prefix, file_name, indicator_1, indicator_2, output_file_type):\n",
    "        '''\n",
    "        Used to create a complete file path for output based on the current date and passed user inputs.\n",
    "        '''\n",
    "        run_date = dt.date.today()\n",
    "        run_date_formatted = run_date.strftime('%Y%m%d')\n",
    "        result_output_file_path_detailed = f'{base_location_value}\\\\{folder_prefix}\\\\{file_name}_{indicator_1}_{indicator_2}_{run_date_formatted}.{output_file_type}'\n",
    "\n",
    "        return result_output_file_path_detailed\n",
    "    \n",
    "    def define_full_output_file_path_generic(self, base_location_value, folder_prefix, file_name, output_file_type):\n",
    "        '''\n",
    "        Used to create a complete file path for a generic output for Tableau datasets\n",
    "        '''\n",
    "        result_output_file_path_generic = f'{base_location_value}\\\\{folder_prefix}\\\\{file_name}.{output_file_type}'\n",
    "\n",
    "        return result_output_file_path_generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessData:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def convert_wide_to_long(self, df_input, name_column_date, desc_input_variable):\n",
    "        '''\n",
    "        Pivot column names to rows to ensure standardized column names are used in code. \n",
    "        This allows the user to change the economic indicators used without updating the underlying code.\n",
    "        Convert the reporting date column from an object to datetime for graphing and calculations.\n",
    "        Set a variable description based on user input for output presentation, as FRED csv does not include descriptions  \n",
    "        '''\n",
    "        df_long = df_input.copy() # create copy of initial dataframe for transformations, as initial table will be output later for dashboard\n",
    "        dict_df_columns = df_input.columns\n",
    "        df_long = df_long.melt(id_vars=dict_df_columns[0], value_vars=dict_df_columns[1])\n",
    "        '''\n",
    "        SQL Equivalent ->\n",
    "        SELECT a.Date, c.variable, c.value\n",
    "        FROM TableA a\n",
    "        UNPIVOT\n",
    "        value for variable IN (columns) as c\n",
    "        '''\n",
    "        df_long['reporting_date'] = pd.to_datetime(df_long[name_column_date], errors='coerce')\n",
    "        df_long['variable_desc'] = str(desc_input_variable)\n",
    "\n",
    "        return df_long\n",
    "    \n",
    "    def stack_and_format(self, df_1, df_2):\n",
    "        '''\n",
    "        Create a stacked dataframe from both data sources for use in the time series chart.\n",
    "        Update column names to be easily understood by business user's in charts and tables.\n",
    "        '''\n",
    "        df_input_stack = pd.concat([df_1, df_2]) # Stack dataframes for plotting\n",
    "        '''\n",
    "        SQL Equivalent ->\n",
    "        SELECT a.* FROM tableA a\n",
    "        UNION ALL\n",
    "        SELECT b.* FROM tableB b\n",
    "        '''\n",
    "        df_input_stack_formatted = pd.DataFrame() # create empty dataframe to store formatted table\n",
    "        df_input_stack_formatted['Date'] = df_input_stack['reporting_date'].dt.strftime('%Y-%m-%d')\n",
    "        df_input_stack_formatted['Value'] = df_input_stack['value']\n",
    "        df_input_stack_formatted['Variable Description'] = df_input_stack['variable_desc']   \n",
    "        '''\n",
    "        SQL Equivalent ->\n",
    "        SELECT \n",
    "        a.[reporting_date] as 'Date'\n",
    "        a.[value] as 'Value'\n",
    "        a.[variable_desc] as 'Variable Description'\n",
    "        FROM tableA a\n",
    "        '''\n",
    "        return df_input_stack_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalysisCalculations:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def calc_correlation(self, df, name_column_x, name_column_y):\n",
    "        '''\n",
    "        Calculate the correlation coefficiant based in the user input datasets.\n",
    "        Create a message to be output after the application finishes running for the user to quickly view the key results.\n",
    "        '''\n",
    "        value_correlation = df[name_column_x].corr(df[name_column_y])\n",
    "        message_result_correlation = f'Correlation coefficient = {value_correlation:.4f}.'\n",
    "\n",
    "        return value_correlation, message_result_correlation\n",
    "    \n",
    "    def calc_correlation_lag(self, df, name_column_x, name_column_y, lag_periods, lag_column_x_or_y):\n",
    "        '''\n",
    "        Shift dataframe column by -1 * lag_periods to calculate lag correlation.\n",
    "        The lag column can be defined as x or y.\n",
    "        '''\n",
    "        valid_lag_columns = ['x', 'y']\n",
    "\n",
    "        if lag_column_x_or_y not in valid_lag_columns:\n",
    "            raise ValueError(f'Lag column value invalid. Valid values are {valid_lag_columns}.')\n",
    "\n",
    "        if not isinstance(lag_periods, int):\n",
    "            raise ValueError(f'Lag periods is not an integer.')\n",
    "                \n",
    "        if lag_column_x_or_y == 'x':\n",
    "            df_lag = df[[name_column_x]].shift(-1*abs(lag_periods))\n",
    "            value_lag_correlation = df_lag[name_column_x].corr(df[name_column_y])\n",
    "        \n",
    "        if lag_column_x_or_y == 'y':\n",
    "            df_lag = df[[name_column_y]].shift(-1)\n",
    "            value_lag_correlation = df_lag[name_column_y].corr(df[name_column_x])\n",
    "        \n",
    "        return value_lag_correlation\n",
    "\n",
    "    def calc_correlation_lead(self, df, name_column_x, name_column_y, lead_periods, lead_column_x_or_y):\n",
    "        '''\n",
    "        Shift defined dataframe column by lead_periods to calculate lead correlation.\n",
    "        The lead column can be defined as x or y.\n",
    "        '''\n",
    "        valid_lead_columns = ['x', 'y']\n",
    "\n",
    "        if lead_column_x_or_y not in valid_lead_columns:\n",
    "            raise ValueError(f'Lead column value invalid. Valid values are {valid_lead_columns}.')\n",
    "\n",
    "        if not isinstance(lead_periods, int):\n",
    "            raise ValueError(f'Lead periods is not an integer.')\n",
    "                \n",
    "        if lead_column_x_or_y == 'x':\n",
    "            df_lead = df[[name_column_x]].shift(abs(lead_periods))\n",
    "            value_lead_correlation = df_lead[name_column_x].corr(df[name_column_y])\n",
    "        \n",
    "        if lead_column_x_or_y == 'y':\n",
    "            df_lead = df[[name_column_y]].shift(abs(lead_periods))\n",
    "            value_lead_correlation = df_lead[name_column_y].corr(df[name_column_x])\n",
    "        \n",
    "        return value_lead_correlation\n",
    "          \n",
    "    def calc_dataset_periods(self, df, name_column_date, desc_input_variable):\n",
    "        '''\n",
    "        Calculate the number of periods, beginning year, and ending year for user review.\n",
    "        Create a message to be output after the application finishes running for the user to quickly view the key results.\n",
    "        '''\n",
    "        number_of_periods = len(df)\n",
    "        year_start = df[name_column_date].min().year\n",
    "        year_end = df[name_column_date].max().year\n",
    "        '''\n",
    "        SQL Equivalent ->\n",
    "        SELECT\n",
    "        a.[variable_desc]\n",
    "        ,COUNT(a.[reporting_date]) as 'number_of_periods'\n",
    "        ,MIN(YEAR(a.[reporting_date])) as 'year_start'\n",
    "        ,MAX(YEAR(a.[reporting_date])) as 'year_end'\n",
    "        FROM tableA a\n",
    "        GROUP BY a.[variable_desc]\n",
    "        '''\n",
    "        message_time_horizon = (f'{desc_input_variable} table has {number_of_periods} records from {year_start} to {year_end}')\n",
    "\n",
    "        return number_of_periods, year_start, year_end, message_time_horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualizationCreation:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def create_time_series_chart(self, df, desc_input_variable_1, desc_input_variable_2):\n",
    "        '''\n",
    "        Create a time series chart based on the indicators in the datasets.\n",
    "        Each line of the time series chart will be a different color based on the indicator.\n",
    "        '''\n",
    "        fig_time_series = px.line(df,\n",
    "                                  x='Date',\n",
    "                                  y='Value',\n",
    "                                  color='Variable Description',\n",
    "                                  title=f'{desc_input_variable_1} & {desc_input_variable_2}'\n",
    "                                  )\n",
    "        \n",
    "        return fig_time_series\n",
    "    \n",
    "    def create_table(self, df):\n",
    "        '''\n",
    "        Create a simple table to view the input data from FRED.\n",
    "        '''\n",
    "        fig_simple_table = pgo.Figure(\n",
    "                            data=[pgo.Table(\n",
    "                            header=dict(values=list(df.columns), \n",
    "                                        fill_color='skyblue', \n",
    "                                        align='left'),\n",
    "                            cells=dict(values=df.transpose().values.tolist(),\n",
    "                                        fill_color='slategray',\n",
    "                                        align='left')\n",
    "                                )                       \n",
    "                             ]\n",
    "                                    )\n",
    "        \n",
    "        return fig_simple_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TableauOutputPrep():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def format_tableau_output(self, df):\n",
    "        df_tableau_output = df.copy()\n",
    "        df_tableau_output['run_date'] = pd.to_datetime(dt.date.today()) # Add the current date as a column for filtering in Tableau\n",
    "        df_tableau_output['created_by'] = os.getlogin() # Add the userID of the person who ran the report for audit trail\n",
    "\n",
    "        return df_tableau_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SET UP CLASSES THAT WILL BE USED IN PROCESS ###\n",
    "SetParameters = SetParameters()\n",
    "PreProcessData = PreProcessData()\n",
    "AnalysisCalculations = AnalysisCalculations()\n",
    "VisualizationCreation = VisualizationCreation()\n",
    "TableauOutputPrep = TableauOutputPrep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source user parameters for use in Notebook run. Comes from tk application interface in .py version\n",
    "base_location = baseLocation\n",
    "input_file_1 = inputFile1\n",
    "input_file_2 = inputFile2\n",
    "input_desc_1 = inputFileDesc1\n",
    "input_desc_2 = inputFileDesc2\n",
    "lag_periods = lagPeriods\n",
    "lead_periods = leadPeriods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FILE LOCATION SETUP ##\n",
    "input_full_file_path_1 = SetParameters.define_full_file_path(base_location, '01_Input', input_file_1, 'csv')\n",
    "input_full_file_path_2 = SetParameters.define_full_file_path(base_location, '01_Input', input_file_2, 'csv')\n",
    "output_file_path = SetParameters.define_file_path(base_location, '02_Output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSQL Equivalent ->\\nSELECT a.*, b.*\\nFROM tableA a\\nINNER JOIN tableB b\\n    ON a.DATE = b.DATE\\n'"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## DATA SOURCING AND PRE-PROCESSING ##\n",
    "\n",
    "# Source csv files as pandas dataframes. These files can be any downloaded file from FRED with the column format DATE, {some_value}  \n",
    "df_input_1 = pd.read_csv(input_full_file_path_1, sep=',')\n",
    "df_input_2 = pd.read_csv(input_full_file_path_2, sep=',')\n",
    "'''\n",
    "SQL EQUIVALENT -> \n",
    "SELECT a.* FROM tableA a;\n",
    "SELECT b.* FROM tableB b;\n",
    "'''\n",
    "\n",
    "# Pre-process data tables\n",
    "df_preproc_1 = PreProcessData.convert_wide_to_long(df_input_1, 'DATE', input_desc_1)\n",
    "df_preproc_2 = PreProcessData.convert_wide_to_long(df_input_2, 'DATE', input_desc_2)\n",
    "\n",
    "# Stack dataframes for plotting and assign formatted values\n",
    "df_preproc_stack_formatted = PreProcessData.stack_and_format(df_preproc_1, df_preproc_2)\n",
    "\n",
    "# Join dataframes to create a wide-format table for period-specific analysis\n",
    "df_preproc_joined = df_preproc_1.merge(df_preproc_2,\n",
    "                                    on='DATE', # date used as unique key\n",
    "                                    how='inner' # only output information where the unique key exists in each dataset, as some datasets are either available on different cadences or from different time periods\n",
    "                                    )\n",
    "'''\n",
    "SQL Equivalent ->\n",
    "SELECT a.*, b.*\n",
    "FROM tableA a\n",
    "INNER JOIN tableB b\n",
    "    ON a.DATE = b.DATE\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CALCULATIONS ##\n",
    "\n",
    "# Calculate correlation coefficient and create message for output\n",
    "results_correlation = AnalysisCalculations.calc_correlation(df_preproc_joined, 'value_x', 'value_y')\n",
    "dict_results_correlation = {'analysis_description': ['Correlation'],\n",
    "                            'analysis_result': [results_correlation[0]],\n",
    "                            'result_data_type': ['float']\n",
    "                            } # add results to dictionary with description and data type\n",
    "df_results_correlation = pd.DataFrame.from_dict(dict_results_correlation, orient='columns') # convert dictionary to dataframe for final output\n",
    "\n",
    "# Calculate number of time periods available for comparison and in each dataset\n",
    "results_time_horizon_1 = AnalysisCalculations.calc_dataset_periods(df_preproc_1, 'reporting_date', input_desc_1)\n",
    "results_time_horizon_2 = AnalysisCalculations.calc_dataset_periods(df_preproc_2, 'reporting_date', input_desc_2)\n",
    "\n",
    "# Calculate lag and lead correlations based on user input lag and lead periods\n",
    "results_lag_correlation_x = AnalysisCalculations.calc_correlation_lag(df_preproc_joined, 'value_x', 'value_y', lag_periods, 'x')\n",
    "results_lag_correlation_y = AnalysisCalculations.calc_correlation_lag(df_preproc_joined, 'value_x', 'value_y', lag_periods, 'y')\n",
    "dict_results_lag = {'analysis_description': [f'Lag Correlation {input_desc_1}', f'Lag Periods {input_desc_1}', f'Lag Correlation {input_desc_2}', f'Lag Periods {input_desc_2}'],\n",
    "                    'analysis_result': [results_lag_correlation_x, lag_periods, results_lag_correlation_y, lag_periods],\n",
    "                    'result_data_type': ['float', 'int', 'float', 'int']\n",
    "                    } # add results to dictionary with description and data type\n",
    "df_results_lag = pd.DataFrame.from_dict(dict_results_lag, orient='columns') # convert dictionary to dataframe for final output\n",
    "\n",
    "results_lead_correlation_x = AnalysisCalculations.calc_correlation_lead(df_preproc_joined, 'value_x', 'value_y', lead_periods, 'x')\n",
    "results_lead_correlation_y = AnalysisCalculations.calc_correlation_lead(df_preproc_joined, 'value_x', 'value_y', lead_periods, 'y')\n",
    "dict_results_lead = {'analysis_description': [f'Lead Correlation {input_desc_1}', f'Lead Periods {input_desc_1}', f'Lead Correlation {input_desc_2}', f'Lead Periods {input_desc_2}'],\n",
    "                    'analysis_result': [results_lead_correlation_x, lead_periods, results_lead_correlation_y, lead_periods],\n",
    "                    'result_data_type': ['float', 'int', 'float', 'int']\n",
    "                    } # add results to dictionary with description and data type\n",
    "df_results_lead = pd.DataFrame.from_dict(dict_results_lead, orient='columns') # convert dictionary to dataframe for final output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CALCULATION RESULTS FORMATTING ##\n",
    "\n",
    "# Create empty dataframe to compile analysis data points into dataframe for csv output\n",
    "df_analysis = pd.DataFrame()\n",
    "df_analysis['analysis_description'] = str()\n",
    "df_analysis['analysis_result'] = str()\n",
    "df_analysis['result_data_type'] = str()\n",
    "\n",
    "# Add calculation results to dataframe\n",
    "df_analysis = pd.concat([df_analysis, df_results_correlation, df_results_lag, df_results_lead])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOTLY FIGURE CREATION ##\n",
    "\n",
    "# Create time series chart plotting the value of each economic indicator over time\n",
    "fig_time_series = VisualizationCreation.create_time_series_chart(df_preproc_stack_formatted, input_desc_1, input_desc_2)\n",
    "\n",
    "# Create table including the calculated analytics values\n",
    "fig_table = VisualizationCreation.create_table(df_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TABLEAU DASHBOARD PREP ##\n",
    "\n",
    "# Add run_date and created_by to stacked table and analysis table for use in tableau time series chart and table\n",
    "df_tableau_output_stacked = TableauOutputPrep.format_tableau_output(df_preproc_stack_formatted)\n",
    "df_tableau_analysis = TableauOutputPrep.format_tableau_output(df_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## OUTPUT STEPS ##\n",
    "\n",
    "# Define output file names\n",
    "output_file_time_series_PDF = SetParameters.define_full_output_file_path_detailed(base_location, '02_Output', 'timeSeries', input_file_1, input_file_2, 'pdf')\n",
    "output_file_time_series_HTML = SetParameters.define_full_output_file_path_detailed(base_location, '02_Output', 'timeSeries', input_file_1, input_file_2, 'html')\n",
    "output_file_table_PDF = SetParameters.define_full_output_file_path_detailed(base_location, '02_Output', 'table', input_file_1, input_file_2, 'pdf')\n",
    "output_file_table_HTML = SetParameters.define_full_output_file_path_detailed(base_location, '02_Output', 'table', input_file_1, input_file_2, 'html')\n",
    "output_file_table_tableau_stacked_csv = SetParameters.define_full_output_file_path_generic(base_location, '02_Output', 'tableau_stacked_table','csv')\n",
    "output_file_table_tableau_analysis_csv = SetParameters.define_full_output_file_path_generic(base_location, '02_Output', 'tableau_analysis_table', 'csv')\n",
    "#output_file_table_tableau_stacked_hyper = SetParameters.define_full_output_file_path_generic(base_location, '02_Output', 'tableau_stacked_table','hyper')\n",
    "#output_file_table_tableau_analysis_hyper = SetParameters.define_full_output_file_path_generic(base_location, '02_Output', 'tableau_analysis_table', 'hyper')\n",
    "\n",
    "# Output HTML files\n",
    "fig_time_series.write_html(output_file_time_series_HTML)\n",
    "fig_table.write_html(output_file_table_HTML)\n",
    "\n",
    "# Output PDFs\n",
    "pio.write_image(fig_time_series, output_file_time_series_PDF, scale=1, width=1100, height=850)\n",
    "pio.write_image(fig_table, output_file_table_PDF, scale=1, width=1100, height=850)\n",
    "\n",
    "# Output CSVs\n",
    "df_tableau_output_stacked.to_csv(output_file_table_tableau_stacked_csv, sep=',', index=False)\n",
    "df_tableau_analysis.to_csv(output_file_table_tableau_analysis_csv, sep=',', index=False)\n",
    "\n",
    "# Output hyper files\n",
    "#pantab.frame_to_hyper(df_tableau_output_stacked, output_file_table_tableau_stacked_hyper, table='tableau_stacked_table')\n",
    "#pantab.frame_to_hyper(df_tableau_analysis, output_file_table_tableau_analysis_hyper, table='tableau_analysis_table' )\n",
    "\n",
    "# Open HTML in browser for user review\n",
    "os.system(f'start {output_file_table_HTML}')\n",
    "os.system(f'start {output_file_time_series_HTML}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectHGV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
